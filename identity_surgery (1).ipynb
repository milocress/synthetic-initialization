{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "identity_surgery.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2NcIbUfNZwER",
        "outputId": "37b21f26-a182-467d-fa7c-91898ac691ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 57.1 MB/s \n",
            "\u001b[?25hCollecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.0-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Collecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 28.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=a5f7e95713362f65fca02bdd0577d3b035a8ed2e29db392dbe3bddfa4f1a4b20\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: matplotlib-inline, jedi, tokenizers, ipython, huggingface-hub, transformers, sentencepiece, ipdb\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.9.0\n",
            "    Uninstalling ipython-7.9.0:\n",
            "      Successfully uninstalled ipython-7.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.9.0 ipdb-0.13.9 ipython-7.34.0 jedi-0.18.1 matplotlib-inline-0.1.6 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install transformers sentencepiece ipdb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb on"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0U_cpP8gTIl",
        "outputId": "63863399-5728-4b1d-9dd4-8da0d7ccd4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer,  T5ForConditionalGeneration\n",
        "import torch\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "HMQeQTHXZxyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72916d8-e347-4053-f9c9-772754c15b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ipdb\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "nNx_yV0Lbu8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeLayerNorm(torch.nn.Module):\n",
        "  def __init__(self, n=1):\n",
        "    super().__init__()\n",
        "    self.n = n\n",
        "\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    return hidden_states * self.n\n",
        "\n",
        "class FakeLMHead(torch.nn.Module):\n",
        "  def __init__(self, embedding_lookup):\n",
        "    super().__init__()\n",
        "    self.embedding = embedding_lookup\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, embed_dims = x.shape\n",
        "    x = x.transpose(1,2)\n",
        "    W = self.embedding.weight.unsqueeze(0).expand(batch_size, *self.embedding.weight.shape)\n",
        "\n",
        "\n",
        "    prob_logits = torch.bmm(W, x)\n",
        "\n",
        "    # log_probs = F.log_softmax(prob_logits, dim=1)\n",
        "\n",
        "    return prob_logits.transpose(1, 2)"
      ],
      "metadata": {
        "id": "V4tVEsNnnCQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  # model.lm_head = FakeLMHead(model.shared)\n",
        "  model.encoder.final_layer_norm = FakeLayerNorm()\n",
        "\n",
        "  from torch.nn.modules.dropout import Dropout\n",
        "  for block in range(6):\n",
        "    M = torch.tensor(np.identity(512)).float()\n",
        "    Z = torch.tensor(np.zeros(512)).float()\n",
        "    # M[M == 0] = -10\n",
        "\n",
        "    model.encoder.block[block].layer[0].SelfAttention.q.weight = torch.nn.Parameter(M)\n",
        "    model.encoder.block[block].layer[0].SelfAttention.k.weight = torch.nn.Parameter(M)\n",
        "    model.encoder.block[block].layer[0].SelfAttention.v.weight = torch.nn.Parameter(M)\n",
        "    model.encoder.block[block].layer[0].SelfAttention.o.weight = torch.nn.Parameter(M)\n",
        "\n",
        "    # model.encoder.block[block].layer[0].dropout = Dropout(0, False)\n",
        "    # model.encoder.block[block].layer[1].dropout = Dropout(0, False)\n",
        "\n",
        "    WI = np.zeros((512, 2048)).T\n",
        "    np.fill_diagonal(WI, 1)\n",
        "    WO = np.zeros((2048, 512)).T\n",
        "    np.fill_diagonal(WO, 1)\n",
        "\n",
        "    WI = torch.tensor(WI).float()\n",
        "    WO = torch.tensor(WO).float()\n",
        "\n",
        "    model.encoder.block[block].layer[1].DenseReluDense.wi.weight = torch.nn.Parameter(WI)\n",
        "    model.encoder.block[block].layer[1].DenseReluDense.wo.weight = torch.nn.Parameter(WO)\n",
        "\n",
        "    # model.encoder.block[block].layer[1].DenseReluDense.dropout = Dropout(0, False)\n",
        "\n",
        "    model.encoder.block[block].layer[0].layer_norm = FakeLayerNorm(0)\n",
        "    model.encoder.block[block].layer[1].layer_norm = FakeLayerNorm(0)\n",
        "\n",
        "  # model.encoder.dropout = Dropout(0, False)"
      ],
      "metadata": {
        "id": "KkcPm-3vehCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "  model.decoder.final_layer_norm = FakeLayerNorm()\n",
        "\n",
        "  for block in range(6):\n",
        "    M = torch.tensor( np.identity(512)).float()\n",
        "    Z = torch.tensor(np.zeros(512)).float()\n",
        "    # M[M == 0] = -10\n",
        "\n",
        "    model.decoder.block[block].layer[0].SelfAttention.q.weight = torch.nn.parameter.Parameter(M)\n",
        "    model.decoder.block[block].layer[0].SelfAttention.k.weight = torch.nn.parameter.Parameter(M)\n",
        "    model.decoder.block[block].layer[0].SelfAttention.v.weight = torch.nn.parameter.Parameter(M)\n",
        "    model.decoder.block[block].layer[0].SelfAttention.o.weight = torch.nn.parameter.Parameter(M)\n",
        "\n",
        "\n",
        "    # model.decoder.block[block].layer[0].dropout = Dropout(0, False)\n",
        "\n",
        "    model.decoder.block[block].layer[1].EncDecAttention.q.weight = torch.nn.parameter.Parameter(M)\n",
        "    model.decoder.block[block].layer[1].EncDecAttention.k.weight = torch.nn.parameter.Parameter(M)\n",
        "    model.decoder.block[block].layer[1].EncDecAttention.v.weight = torch.nn.parameter.Parameter(M)\n",
        "    model.decoder.block[block].layer[1].EncDecAttention.o.weight = torch.nn.parameter.Parameter(M)\n",
        "\n",
        "    # model.decoder.block[block].layer[1].dropout = Dropout(0, False)\n",
        "    # model.decoder.block[block].layer[2].dropout = Dropout(0, False)\n",
        "    # model.decoder.block[block].layer[2].DenseReluDense.dropout = Dropout(0, False)\n",
        "\n",
        "\n",
        "    WI = np.zeros((512, 2048)).T\n",
        "    np.fill_diagonal(WI, 1)\n",
        "    WO = np.zeros((2048, 512)).T\n",
        "    np.fill_diagonal(WO, 1)\n",
        "\n",
        "    WI = torch.tensor(WI).float()\n",
        "    WO = torch.tensor(WO).float()\n",
        "\n",
        "    model.decoder.block[block].layer[2].DenseReluDense.wi.weight = torch.nn.Parameter(WI)\n",
        "    model.decoder.block[block].layer[2].DenseReluDense.wo.weight = torch.nn.Parameter(WO)\n",
        "\n",
        "    model.decoder.block[block].layer[0].layer_norm = FakeLayerNorm(0)\n",
        "    model.decoder.block[block].layer[1].layer_norm = FakeLayerNorm(0)\n",
        "    model.decoder.block[block].layer[2].layer_norm = FakeLayerNorm(0)\n",
        "\n",
        "\n",
        "  # model.decoder.dropout = Dropout(0, False)"
      ],
      "metadata": {
        "id": "vQP56wxwhlUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_embedding(embedding):\n",
        "  return model.lm_head(embedding)\n",
        "\n",
        "def get_model_output(model, input_string):\n",
        "  tokens = tokenizer(input_string, return_tensors=\"pt\").input_ids\n",
        "  embeds = get_model_embedding_output(tokens)[0]\n",
        "  outtok = torch.argmax(reverse_embedding(embeds), dim=1) \n",
        "  output = tokenizer.decode(outtok, skip_special_tokens=True)\n",
        "  return output\n",
        "\n",
        "def get_model_output_tokens(model, input_string):\n",
        "  tokens = tokenizer(input_string, return_tensors=\"pt\").input_ids\n",
        "  output = model.generate(tokens, num_beams=1, do_sample=False)[0]\n",
        "  return output"
      ],
      "metadata": {
        "id": "EkDt97nBA3Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoder_output(embeddings, block=0):\n",
        "  encoder = model.encoder\n",
        "  return encoder(embeddings)\n",
        "\n",
        "def get_decoder_output(embeddings):\n",
        "  decoder = model.decoder\n",
        "  return decoder(inputs_embeds=embeddings)\n",
        "\n",
        "def get_model_embedding_output(tokens):\n",
        "  encoded_embeddings = get_encoder_output(tokens).last_hidden_state\n",
        "  decoded_embeddings = get_decoder_output(encoded_embeddings)\n",
        "  return decoded_embeddings.last_hidden_state\n"
      ],
      "metadata": {
        "id": "czR_BhmDEHJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_is_identity(model, input_string):\n",
        "  out_string = get_model_output(model, input_string)\n",
        "  return out_string, out_string == input_string\n",
        "\n",
        "input_strings = [\n",
        "    'hello, world!',\n",
        "    'lol',\n",
        "    'abcdefghijklmnopqrstuvwxyz',\n",
        "    'what',\n",
        "    'what is hello in french',\n",
        "    'Translate French to German: \"Bonjour, Monsieur!\"',\n",
        "    \"Hello, hacklodge!! zzyxwvutsrqponmlkjihgfedcba\"\n",
        "]\n",
        "\n",
        "def run_test(test_fun, input_strings):\n",
        "  num_passed = 0\n",
        "  num_failed = 0\n",
        "  for input_string in input_strings:\n",
        "    out_string, is_pass = (test_fun(model, input_string))\n",
        "    num_passed += 1 if is_pass else 0\n",
        "    num_failed += 0 if is_pass else 1\n",
        "    print(f\"input_string: '{input_string}' \\nout_string:   '{out_string}'\")\n",
        "\n",
        "  print(f\"num passed: {num_passed}, num failed: {num_failed}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PdK4LBXKn0SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_test(model_is_identity, input_strings)"
      ],
      "metadata": {
        "id": "wGu1lvCey01u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3996277c-b215-4e9b-9fc6-35a00ac9fe24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_string: 'hello, world!' \n",
            "out_string:   'hello, world!'\n",
            "input_string: 'lol' \n",
            "out_string:   'lol'\n",
            "input_string: 'abcdefghijklmnopqrstuvwxyz' \n",
            "out_string:   'abcdefghijklmnopqrstuvwxyz'\n",
            "input_string: 'what' \n",
            "out_string:   'what'\n",
            "input_string: 'what is hello in french' \n",
            "out_string:   'what is hello in french'\n",
            "input_string: 'Translate French to German: \"Bonjour, Monsieur!\"' \n",
            "out_string:   'Translate French to German: \"Bonjour, Monsieur!\"'\n",
            "input_string: 'Hello, hacklodge!! zzyxwvutsrqponmlkjihgfedcba' \n",
            "out_string:   'Hello, hacklodge!! zzyxwvutsrqponmlkjihgfedcba'\n",
            "num passed: 7, num failed: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(input_string):\n",
        "  return tokenizer(input_string, return_tensors='pt').input_ids\n",
        "\n",
        "def get_embeddings(input_ids, is_encoder=True):\n",
        "  submodule = model.encoder if is_encoder else model.decoder\n",
        "  return submodule.embed_tokens(input_ids)\n",
        "\n",
        "def get_attention_head_output(embeddings, block=0, is_encoder=True):\n",
        "  submodule = model.encoder if is_encoder else model.decoder\n",
        "  return submodule.block[block].layer[0].SelfAttention(embeddings)[0]\n",
        "\n",
        "def get_attention_head_parameters(block=0, is_encoder=True):\n",
        "  submodule = model.encoder if is_encoder else model.decoder\n",
        "  return list(model.encoder.block[block].layer[0].SelfAttention.parameters())\n",
        "\n",
        "def get_xattention_head_output(embeddings, block=0):\n",
        "  submodule = model.decoder # encoder doesn't have cross attention\n",
        "  return submodule.block[block].layer[1].EncDecAttention(embeddings)\n",
        "\n",
        "def get_ff_output(embeddings, block=0, is_encoder=True):\n",
        "  submodule = model.encoder if is_encoder else model.decoder\n",
        "  return model.encoder.block[block].layer[1](embeddings)\n",
        "\n",
        "def get_block_output(embeddings, block=0, is_encoder=True):\n",
        "  submodule = model.encoder if is_encoder else model.decoder\n",
        "  return model.encoder.block[block](embeddings)\n",
        "\n",
        "def get_stack_output(embeddings=None, tokens=None, is_encoder = True):\n",
        "  submodule = model.encoder if is_encoder else model.decoder\n",
        "  return submodule(tokens) if is_encoder else submodule(inputs_embeds=embeddings)\n"
      ],
      "metadata": {
        "id": "1Gc-DH57aKrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = get_tokens(\"'I will take the Ring', he said, 'though I do not know the way'\")\n",
        "embeddings = get_embeddings(tokens)\n",
        "print(\"embeddings:\")\n",
        "print(embeddings)\n",
        "# print(\"attention head output\")\n",
        "# print(get_attention_head_output(embeddings))\n",
        "# print(\"block output:\")\n",
        "# print(get_block_output(embeddings))\n",
        "# print(\"second output shape\")\n",
        "# print(get_block_output(embeddings)[1].shape)\n",
        "# print(\"encoder output compared to embeddings:\")\n",
        "# print(embeddings)\n",
        "# print(get_encoder_output(tokens).last_hidden_state)\n",
        "# print(get_stack_output(embeddings=embeddings, is_encoder=False).last_hidden_state)\n",
        "# print(get_decoder_output(embeddings))\n",
        "# print(reverse_embedding(embeddings))\n",
        "# print(get_model_output(model, \"for other values\").shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36UpuNArBSf7",
        "outputId": "40589ff2-a04f-48c9-f170-781414c5286e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings:\n",
            "tensor([[[ 11.2500,   8.0625,  14.1875,  ...,   9.8125,  -7.8750,  -3.6094],\n",
            "         [ -0.2656, -10.0000,   9.5000,  ...,   3.3438,   9.8125,  -1.9922],\n",
            "         [  1.8828,   6.0000, -16.7500,  ..., -47.5000,  -7.0312,  -6.9375],\n",
            "         ...,\n",
            "         [ 16.2500,  -4.0625,  12.1250,  ...,   9.3125,  -9.0625,  -1.0938],\n",
            "         [ -0.2656, -10.0000,   9.5000,  ...,   3.3438,   9.8125,  -1.9922],\n",
            "         [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.tie_word_embeddings"
      ],
      "metadata": {
        "id": "4qpfIjrorQaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.children"
      ],
      "metadata": {
        "id": "5aehnR17hPpy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "62ddb212-3e5d-41d0-f048-d1d4d08a830f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a11ce1514a2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.zeros((2, 4))\n",
        "np.fill_diagonal(a, 1)\n",
        "b = np.zeros((4, 2))\n",
        "np.fill_diagonal(b, 1)\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "wC6NhHsaaA21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(model.parameters()))"
      ],
      "metadata": {
        "id": "3A64ob9bbxsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(dir)"
      ],
      "metadata": {
        "id": "uROFIyUHbz3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Softmax\n",
        "import torch"
      ],
      "metadata": {
        "id": "9AgGSvCXeesy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHN3ZM53V7iP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}